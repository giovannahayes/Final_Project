<html>
<head>
<style>
body {
  background-color: lightblue;
}
</style>
</head>
</body>

<h1>Team Devas</h1>
  <h2>Loan Prediction Project</h2>
  <h3>Machine Learning Models</h3>

  <h3>Leveraging ML</h3>

  <p>Machine learning can extract deep, complex insights out of data to help make decisions. In many cases, using more advanced models delivers real business value through significantly improving on traditional regression models. The growing adoption of ML and deep learning (DL) prediction models is due to their ability to take advantage of large datasets to learn patterns with higher accuracy than many prior techniques.These models, such as random forests, may embody thousands of decision points and deep learning models take that to the extreme, feeding data through multiple non-linear tensor transformations with millions of parameters derived iteratively from the training data. Simplifying the explanations of ML and DL classifiers has become important to simplify the complexity and making these more accurate techniques adoptable in practice of predicting loan delinquency. Our team compared the complexity,accuracy and impact of random forest, SVM and deep learning models to understand which would be the most applicable model to choose to use for our web app. The notes below are our thoughts on the comparison of the models. 

    </p>


  <article>
    <a target="_blank" href="assets/images/mlgif.gif">
      <img src="assets/images/mlgif.gif" alt="Cloudy Map">
    </a>
    </article>

    <h3>Random Forest and SVM</h3>

    <p>We felt that random forests was the "worry-free" approach, if such a thing exists in ML: There were few hyperparameters to tune with the  exception of for the number of trees. We found the more trees we layered, the more accurate our model became,. . On the contrary, there are a lot of knobs to be turned in SVMs: Choosing the "right" kernel, regularization penalties and the slack variable layered complexity in configuring the model. 
      Both the random forests’ and SVMs’ complexity grows as one increases the number of training samples. Also, we found it can be easy to end up with a lot of support vectors in SVMs and in the worst-case scenario, we could have as many support vectors as we have samples in the training set. Typically with SVMs one has to train an SVM for each class and in contrast random forests can handle multiple classes straight out the gate.
      In short, the complexity of a random forest grows with the number of trees in the forest, and the number of training samples available. We conclude that SVMs are great for relatively small data sets with fewer outliers. Random forests may require more data but they almost always come up with a pretty robust model and we found random forests are much simpler to train and it's easier to find a good, robust model.
      
  
      </p>
  
  
    <article>
      <a target="_blank" href="./assets/images/svm.png">
        <img src="./assets/images/svm.png" alt="Cloudy Map">
      </a>
      </article>


    
    
      <article>
      <a target="_blank" href="./assets/images/rf.png">
        <img src="./assets/images/rf.png" alt="Cloudy Map">
      </a>
      </article>
    

      <h3>Deep Learning</h3>

      <p>Deep learning algorithms require "relatively" large datasets to work well. Also, deep learning algorithms require much more experience: Setting up a neural network using deep learning algorithms is more tedious than using classifiers such as random forests and SVMs. On the other hand, deep learning really shines when it comes to solving complex problems. Deep neural networks are really good at detecting outliers on complex and highly unstructured data. Another advantage is that you have to worry less about the feature engineering piece. The one detractor and difficulty lies in the fact that deep learning models used within regulated financial markets need to have a certain level of explainability. Regulators require lenders to present sound arguments on why the credit decision is considered fair and unbiased however deep neural networks act as black-box models and data scientists find it hard to explain the reasons behind an output. Again, in practice, the decision we found that deciding which classifier to choose really depends on our dataset and the general level complexity of our problem. In practice we decided to start with the simplest model and continued to try more complex models until we found the model with the highest degree of accuracy.
    
        </p>
    
    
      <article>
        <a target="_blank" href="./assets/images/dnn.png">
          <img src="./assets/images/dnn.png" alt="Cloudy Map">
        </a>
        </article>

        <p>(Caruana, 2006)
        <p>(Raschka)

  <!-- <img src="assets/images/Alicia.png" alt="Awesome" width="25%"> -->